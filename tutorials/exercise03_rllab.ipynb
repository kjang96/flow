{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 03: Running rllab Experiments\n",
    "\n",
    "This tutorial walks you through the process of running traffic simulations in Flow with rllab-powered agents. Autonomous agents will learn to maximize a certain reward over the rollouts, using the **rllab** library. Simulations of this form will depict the propensity of RL agents to influence the traffic of a human fleet in order to make the whole fleet more efficient (for some given metrics). \n",
    "\n",
    "In this exercise, we simulate an initially perturbed single lane ring road, where we introduce a single autonomous vehicle. We witness that, after some training, that the autonomous vehicle learns to avoid the \"phantom jams\" which form when only human dynamics are involved.\n",
    "\n",
    "## 1. Components of a Simulation\n",
    "All simulations, both in the presence and absence of RL, require two components: a *scenario*, and an *environment*. Scenarios describe the features of the transportation network used in simulation. This includes the positions and properties of nodes and edges constituting the lanes and junctions, as well as properties of the vehicles, traffic lights, inflows, etc... in the network. Environments, on the other hand, initialize, reset, and advance simulations, and act as the primary interface between the reinforcement learning algorithm and the scenario. Moreover, custom environments may be used to modify the dynamical features of an scenario. Finally, in the RL case, it is in the *environment* that the state/action spaces and the reward function are defined. \n",
    "\n",
    "## 2. Setting up a Scenario\n",
    "Flow contains a plethora of pre-designed scenarios used to replicate highways, intersections, and merges in both closed and open settings. All these scenarios are located in flow/scenarios. In order to recreate a ring road network, we begin by importing the scenario `LoopScenario`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from flow.scenarios.loop.loop_scenario import LoopScenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This scenario, as well as all other scenarios in Flow, are parameterized by the following arguments: \n",
    "* name\n",
    "* generator_class\n",
    "* vehicles\n",
    "* net_params\n",
    "* initial_config\n",
    "* traffic_lights\n",
    "\n",
    "These parameters allow a single scenario to be recycled for multitude of different network settings. For example, `LoopScenario` may be used to create ring roads of variable length with a variable number of lanes and vehicles.\n",
    "\n",
    "### 2.1 Name\n",
    "The `name` argument is a string variable depicting the name of the scenario. This has no effect on the type of network created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name = \"ring_example\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Generator Class\n",
    "Generator classes are used to create configuration and net files needed to initialize a simulation instance. The methods of this class are called by the base scenario class. All scenarios in Flow are come with an analogous generator that are located in the same directory as the scenario. In the case of `LoopScenario`, this generator is called `CircleGenerator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from flow.scenarios.loop.gen import CircleGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Vehicles\n",
    "The `Vehicles` class stores state information on all vehicles in the network. This class is used to identify the dynamical features of a vehicle and whether it is controlled by a reinforcement learning agent. Morover, information pertaining to the observations and reward function can be collected from various `get` methods within this class.\n",
    "\n",
    "The initial configuration of this class describes the number of vehicles in the network at the start of every simulation, and specifies the characteristic features of these vehicles. We begin by creating an empty `Vehicles` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from flow.core.vehicles import Vehicles\n",
    "\n",
    "vehicles = Vehicles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this object is created, vehicles may be introduced using the `add` method. This method specifies the types and quantities of vehicles at the start of a simulation rollout. For a description of the various arguements associated with the `add` method, we refer the reader to the following documentation (reference readthedocs).\n",
    "\n",
    "When adding vehicles, their dynamical behaviors may be specified either (default) by the simulator, or by a user-generated model. For longitudonal (acceleration) dynamics, several prominent car-following models are implemented in Flow. For this example, the acceleration behavior of all vehicles will be defined by the Intelligent Driver Model (IDM) [2]. As such, we import the `IDMController` to specify the longitudinal dynamics of human-controlled cars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from flow.controllers.car_following_models import IDMController"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another controller we will need to define is for the vehicle's routing behavior. For closed network where the route for any vehicle is repeated, the `ContinuousRouter` controller is used to pertpetually reroute all vehicles to the initial set route."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from flow.controllers.routing_controllers import ContinuousRouter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add 13 vehicles of type \"human\" with the above longitudinal and routing behavior into the `Vehicles` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vehicles.add(\"human\",\n",
    "             acceleration_controller=(IDMController, {}),\n",
    "             routing_controller=(ContinuousRouter, {}),\n",
    "             num_vehicles=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to add RL-powered vehicles (only one in this case). We do something similar to before, with one change: we import Flow's `RLController`, which applies actions returned by the reinforcement learning algorithm (as specified in the environment's action space and `apply_rl_actions` method). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from flow.controllers.rlcontroller import RLController\n",
    "    \n",
    "vehicles.add(veh_id=\"rl\",\n",
    "             acceleration_controller=(RLController, {}),\n",
    "             routing_controller=(ContinuousRouter, {}),\n",
    "             num_vehicles=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 NetParams\n",
    "\n",
    "`NetParams` are network-specific parameters used to define the shape and properties of a network. Unlike most other parameters, `NetParams` may vary drastically dependent on the specific network configuration, and accordingly most parameters are stored in the `additional_params` attribute. In order to determine which `additional_params` variable may be needed for a specific scenario, we refer to the `ADDITIONAL_NET_PARAMS` variable located in the scenario file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lanes': 1, 'length': 230, 'resolution': 40, 'speed_limit': 30}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flow.scenarios.loop.loop_scenario import ADDITIONAL_NET_PARAMS\n",
    "\n",
    "ADDITIONAL_NET_PARAMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the `ADDITIONAL_NET_PARAMS` dict from the ring road scenario, we see that the required parameters are:\n",
    "\n",
    "* **length**: length of the ring road\n",
    "* **lanes**: number of lanes\n",
    "* **speed**: speed limit for all edges\n",
    "* **resolution**: resolution of the curves on the ring. Setting this value to 1 converts the ring to a diamond.\n",
    "\n",
    "\n",
    "At times, other inputs (for example `no_internal_links`) may be needed by the scenario to recreate proper network features/behavior. These requirements can be founded in the scenario's documentation. Furthermore, for this exercise, we use the scenario's default parameters when creating the NetParams object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from flow.core.params import NetParams\n",
    "\n",
    "net_params = NetParams(additional_params=ADDITIONAL_NET_PARAMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 InitialConfig\n",
    "\n",
    "`InitialConfig` specifies parameters that affect the positioning of vehicles in the network at the start of a simulation. These parameters can be used to limit the edges and number of lanes vehicles originally occupy, and provide a means of adding randomness to the starting positions of vehicles. In order to introduce a small initial disturbance to the system of vehicles in the network, we set the `perturbation` term in `InitialConfig` to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from flow.core.params import InitialConfig\n",
    "\n",
    "initial_config = InitialConfig(spacing=\"uniform\", perturbation=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 TrafficLights\n",
    "\n",
    "TrafficLights are used to desribe the positions and types of traffic lights in the network. These inputs are outside the scope of this tutorial, and instead are covered in `exercise06_traffic_lights.ipynb`. For our example, we create an empty `TrafficLights` object, ensuring that none are placed on any nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from flow.core.traffic_lights import TrafficLights\n",
    "\n",
    "traffic_lights = TrafficLights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setting up an Environment\n",
    "\n",
    "Several environments in Flow exist to train RL agents of different forms (e.g. autonomous vehicles, traffic lights) to perform a variety of different tasks. These environments are often scenario or task specific; however, some can be deployed on an ambiguous set of scenarios as well. One such enviornment, `AccelEnv`, may be used to train a variable number of vehicles in a fully observable network with a *static* number of vehicles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.envs.loop.loop_accel import AccelEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of an environment allows us to view the cumulative reward simulation rollouts receive, along with to specify the state/action spaces.\n",
    "\n",
    "Envrionments in Flow are parametrized by three components:\n",
    "* env_params\n",
    "* sumo_params\n",
    "* scenario\n",
    "\n",
    "### 3.1 SumoParams\n",
    "`SumoParams` specifies simulation-specific variables. These variables include the length of any simulation step and whether to render the GUI when running the experiment. For this example, we consider a simulation step length of 0.1s and activate the GUI. \n",
    "\n",
    "**Note** For training purposes, it is highly recommanded to deactivate the GUI in order to avoid global slow down. In such case, one just need to specify the following: `sumo_binary=\"sumo\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from flow.core.params import SumoParams\n",
    "\n",
    "# sumo_params = SumoParams(sim_step=0.1, sumo_binary=\"sumo-gui\")\n",
    "sumo_params = SumoParams(sim_step=0.1, sumo_binary=\"sumo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 EnvParams\n",
    "\n",
    "`EnvParams` specifies environment and experiment-specific parameters that either affect the training process or the dynamics of various components within the scenario. Much like `NetParams`, an experiment demands to specify some additional parameters that are specific to a given experience. In this case, we want to specify the scenario used (`LoopScenario`) plus the `target_velocity`. The latter is basically the velocity that the autonomous vehicles will strive to impose to the whole fleet. Most generally, the only possible actions of autonomous vehicles will be to decelerate/accelerate (this comes from the action space in `AccelEnv`). It is also here that we want to specify the range of acceleration that they are capable of. Finally, it is important to specify here the *HORIZON* of the experiment, which is the duration of one episode (during which the RL-agent acquire data). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import EnvParams\n",
    "\n",
    "HORIZON = 100\n",
    "\n",
    "additional_env_params = {\"target_velocity\":8,\n",
    "                         \"scenario_type\":LoopScenario,\n",
    "                         \"ring_length\":[220, 270],\n",
    "                         \"max_decel\":1,\n",
    "                         \"max_accel\":1}\n",
    "\n",
    "env_params = EnvParams(horizon=HORIZON, additional_params=additional_env_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Setting up an Experiment\n",
    "\n",
    "First, we need to define the scenario that we'll use: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the scenario object\n",
    "scenario = LoopScenario(name=\"ring_example\",\n",
    "                        generator_class=CircleGenerator,\n",
    "                        vehicles=vehicles,\n",
    "                        net_params=net_params,\n",
    "                        initial_config=initial_config,\n",
    "                        traffic_lights=traffic_lights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have to specify our Gym Environment and the algorithm that our RL agents we'll use.\n",
    "To specify the environment, one has to use the environment's name (a simple string). A list of all environment names is located in `flow/envs/__init__.py`.\n",
    "To create the Gym Environment, the only necessary parameters are the environment name plus the previously defined variables.\n",
    "In this experiment, we use a Gaussian MLP policy: we just need to specify its dimensions `(32,32)` and the environment name. We'll use linear baselines and the Trust Region Policy Optimization (TRPO) algorithm (see https://arxiv.org/abs/1502.05477):\n",
    "- The `batch_size` parameter specifies the size of the batch during one step of the gradient descent. \n",
    "- The `max_path_length` parameter indicates the biggest rollout size possible of the experiment. \n",
    "- The `n_itr` parameter gives the number of iterations used in training the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nishant/anaconda3/envs/flow/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n",
      "/Users/nishant/anaconda3/envs/flow/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-04 13:42:12.606202 PDT | Warning: skipping Gym environment monitoring since snapshot_dir not configured.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nishant/anaconda3/envs/flow/lib/python3.5/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nishant/Development/research/rllab-multiagent/rllab/envs/gym_env.py\", line 11, in <module>\n",
      "    from gym.wrappers.monitoring import logger as monitor_logger\n",
      "ImportError: cannot import name 'logger'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-04 13:42:13.661876 PDT | observation space: Box(4,)\n",
      "2018-05-04 13:42:13.663054 PDT | action space: Box(1,)\n",
      "2018-05-04 13:42:14.710899 PDT | Populating workers...\n",
      "2018-05-04 13:42:14.712704 PDT | Populated\n",
      "\n",
      "-----------------------\n",
      "ring length: 249\n",
      "v_max: 11.613727166217824\n",
      "-----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0% [###                           ] 100% | ETA: 00:00:25"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------\n",
      "ring length: 258\n",
      "v_max: 12.25853090863283\n",
      "-----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0% [######                        ] 100% | ETA: 00:00:22"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------\n",
      "ring length: 235\n",
      "v_max: 10.593260524824172\n",
      "-----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0% [#########                     ] 100% | ETA: 00:00:19"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------\n",
      "ring length: 257\n",
      "v_max: 12.187368194492763\n",
      "-----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0% [############                  ] 100% | ETA: 00:00:16"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------\n",
      "ring length: 247\n",
      "v_max: 11.469173128137612\n",
      "-----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0% [###############               ] 100% | ETA: 00:00:13"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------\n",
      "ring length: 244\n",
      "v_max: 11.251538419220173\n",
      "-----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0% [##################            ] 100% | ETA: 00:00:11"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------\n",
      "ring length: 221\n",
      "v_max: 9.555454406750316\n",
      "-----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0% [#####################         ] 100% | ETA: 00:00:08"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------\n",
      "ring length: 241\n",
      "v_max: 11.03297886088859\n",
      "-----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0% [########################      ] 100% | ETA: 00:00:05"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------\n",
      "ring length: 255\n",
      "v_max: 12.044671048467864\n",
      "-----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0% [###########################   ] 100% | ETA: 00:00:02"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------\n",
      "ring length: 270\n",
      "v_max: 13.102160898070554\n",
      "-----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0% [##############################] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-04 13:42:42.345154 PDT | itr #0 | fitting baseline...\n",
      "2018-05-04 13:42:42.385677 PDT | itr #0 | fitted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:00:27\n",
      "/Users/nishant/Development/research/rllab-multiagent/rllab/baselines/linear_feature_baseline.py:41: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  featmat.T.dot(returns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m=: Compiling function f_loss\u001b[0m\n",
      "\u001b[35mdone in 11.401 seconds\u001b[0m\n",
      "\u001b[35m=: Compiling function constraint\u001b[0m\n",
      "\u001b[35mdone in 2.219 seconds\u001b[0m\n",
      "2018-05-04 13:42:56.023931 PDT | itr #0 | computing loss before\n",
      "2018-05-04 13:42:56.031837 PDT | itr #0 | performing update\n",
      "2018-05-04 13:42:56.032869 PDT | itr #0 | computing descent direction\n",
      "\u001b[35m=: Compiling function f_grad\u001b[0m\n",
      "\u001b[35mdone in 16.651 seconds\u001b[0m\n",
      "\u001b[35m=: Compiling function f_Hx_plain\u001b[0m\n",
      "\u001b[35mdone in 18.989 seconds\u001b[0m\n",
      "2018-05-04 13:43:32.012524 PDT | itr #0 | descent direction computed\n",
      "\u001b[35m=: Compiling function f_loss_constraint\u001b[0m\n",
      "\u001b[35mdone in 2.866 seconds\u001b[0m\n",
      "2018-05-04 13:43:34.946434 PDT | itr #0 | backtrack iters: 10\n",
      "2018-05-04 13:43:34.947732 PDT | itr #0 | computing loss after\n",
      "2018-05-04 13:43:34.948722 PDT | itr #0 | optimization finished\n",
      "2018-05-04 13:43:34.966746 PDT | itr #0 | saving snapshot...\n",
      "2018-05-04 13:43:34.968953 PDT | itr #0 | saved\n",
      "2018-05-04 13:43:34.972359 PDT | -----------------------  ---------------\n",
      "2018-05-04 13:43:34.974154 PDT | Iteration                    0\n",
      "2018-05-04 13:43:34.975326 PDT | AverageDiscountedReturn  -1793.7\n",
      "2018-05-04 13:43:34.976605 PDT | AverageReturn            -2284.59\n",
      "2018-05-04 13:43:34.978170 PDT | ExplainedVariance            3.64153e-14\n",
      "2018-05-04 13:43:34.980476 PDT | NumTrajs                    10\n",
      "2018-05-04 13:43:34.982158 PDT | Entropy                      1.41894\n",
      "2018-05-04 13:43:34.985675 PDT | Perplexity                   4.13273\n",
      "2018-05-04 13:43:34.987601 PDT | StdReturn                   44.9838\n",
      "2018-05-04 13:43:34.989456 PDT | MaxReturn                -2205.75\n",
      "2018-05-04 13:43:34.991997 PDT | MinReturn                -2329.26\n",
      "2018-05-04 13:43:34.993607 PDT | AveragePolicyStd             1\n",
      "2018-05-04 13:43:34.995369 PDT | LossBefore                   6.59028e-17\n",
      "2018-05-04 13:43:34.997015 PDT | LossAfter                   -0.000758916\n",
      "2018-05-04 13:43:34.998694 PDT | MeanKLBefore                 0\n",
      "2018-05-04 13:43:35.002040 PDT | MeanKL                       0.00614834\n",
      "2018-05-04 13:43:35.005995 PDT | dLoss                        0.000758916\n",
      "2018-05-04 13:43:35.007545 PDT | -----------------------  ---------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rllab.algos.trpo import TRPO\n",
    "from rllab.baselines.linear_feature_baseline import LinearFeatureBaseline\n",
    "from rllab.policies.gaussian_mlp_policy import GaussianMLPPolicy\n",
    "from rllab.envs.normalized_env import normalize\n",
    "from rllab.envs.gym_env import GymEnv\n",
    "\n",
    "env_name = \"WaveAttenuationPOEnv\"\n",
    "pass_params = (env_name, sumo_params, vehicles, env_params, net_params,\n",
    "                   initial_config, scenario)\n",
    "\n",
    "env = GymEnv(env_name, record_video=False, register_params=pass_params)\n",
    "horizon = env.horizon\n",
    "env = normalize(env)\n",
    "\n",
    "policy = GaussianMLPPolicy(\n",
    "        env_spec=env.spec,\n",
    "        hidden_sizes=(32, 32)\n",
    "    )\n",
    "\n",
    "baseline = LinearFeatureBaseline(env_spec=env.spec)\n",
    "\n",
    "algo = TRPO(\n",
    "        env=env,\n",
    "        policy=policy,\n",
    "        baseline=baseline,\n",
    "        batch_size=3600 * 72 * 2,\n",
    "        discount=0.999,\n",
    "        max_path_length=horizon,\n",
    "        n_itr=5,\n",
    "        # whole_paths=True,\n",
    "        # step_size=v[\"step_size\"],\n",
    "    )\n",
    "algo.train(),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. The whole code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we regroup all the previous commands in one single cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from rllab.envs.normalized_env import normalize\n",
    "from rllab.misc.instrument import stub, run_experiment_lite\n",
    "from rllab.algos.trpo import TRPO\n",
    "from rllab.baselines.linear_feature_baseline import LinearFeatureBaseline\n",
    "from rllab.policies.gaussian_mlp_policy import GaussianMLPPolicy\n",
    "\n",
    "# recurrent stuff\n",
    "from rllab.policies.gaussian_gru_policy import GaussianGRUPolicy\n",
    "from rllab.optimizers.conjugate_gradient_optimizer import ConjugateGradientOptimizer, FiniteDifferenceHvp\n",
    "\n",
    "from flow.scenarios.loop.gen import CircleGenerator\n",
    "from flow.scenarios.loop.loop_scenario import LoopScenario\n",
    "from flow.controllers.rlcontroller import RLController\n",
    "from flow.controllers.lane_change_controllers import *\n",
    "from flow.controllers.car_following_models import *\n",
    "from flow.controllers.routing_controllers import *\n",
    "from flow.core.vehicles import Vehicles\n",
    "from flow.core.params import *\n",
    "from rllab.envs.gym_env import GymEnv\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "HORIZON = 10\n",
    "\n",
    "\n",
    "def run_task(v):\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    sumo_params = SumoParams(sim_step=0.1, sumo_binary=\"sumo-gui\", seed=0)\n",
    "\n",
    "    vehicles = Vehicles()\n",
    "    vehicles.add(veh_id=\"rl\",\n",
    "                 acceleration_controller=(RLController, {}),\n",
    "                 routing_controller=(ContinuousRouter, {}),\n",
    "                 num_vehicles=1)\n",
    "    vehicles.add(veh_id=\"idm\",\n",
    "                 acceleration_controller=(IDMController, {}),\n",
    "                 routing_controller=(ContinuousRouter, {}),\n",
    "                 num_vehicles=21)\n",
    "\n",
    "\n",
    "    additional_env_params = {\"target_velocity\": 8,\n",
    "                             \"scenario_type\": LoopScenario,\n",
    "                             \"ring_length\":[220, 270],\n",
    "                             \"max_decel\":1,\n",
    "                             \"max_accel\":1}\n",
    "    env_params = EnvParams(horizon=HORIZON,\n",
    "                           additional_params=additional_env_params)\n",
    "\n",
    "    additional_net_params = {\"length\": 260, \"lanes\": 1, \"speed_limit\": 30,\n",
    "                             \"resolution\": 40}\n",
    "    net_params = NetParams(additional_params=additional_net_params)\n",
    "\n",
    "    initial_config = InitialConfig(spacing=\"uniform\", bunching=50)\n",
    "\n",
    "    print(\"XXX name\", exp_tag)\n",
    "    scenario = LoopScenario(exp_tag, CircleGenerator, vehicles, net_params,\n",
    "                            initial_config=initial_config)\n",
    "\n",
    "    env_name = \"WaveAttenuationPOEnv\"\n",
    "    pass_params = (env_name, sumo_params, vehicles, env_params, net_params,\n",
    "                   initial_config, scenario)\n",
    "\n",
    "    env = GymEnv(env_name, record_video=False, register_params=pass_params)\n",
    "    horizon = env.horizon\n",
    "    env = normalize(env)\n",
    "\n",
    "    policy = GaussianMLPPolicy(\n",
    "        env_spec=env.spec,\n",
    "        hidden_sizes=(32, 32)\n",
    "    )\n",
    "\n",
    "    baseline = LinearFeatureBaseline(env_spec=env.spec)\n",
    "\n",
    "    algo = TRPO(\n",
    "        env=env,\n",
    "        policy=policy,\n",
    "        baseline=baseline,\n",
    "        batch_size=3600 * 72 * 2,\n",
    "        max_path_length=horizon,\n",
    "        discount=0.999,\n",
    "        n_itr=5,\n",
    "        # whole_paths=True,\n",
    "        # step_size=v[\"step_size\"],\n",
    "    )\n",
    "    algo.train(),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Running the experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last few parameters to specify are : \n",
    "- The `n_parallel` cores you want to use for your experiment. If you set `n_parallel=2`, two processors will execute your code in parallel which results in a global roughly linear speed-up.\n",
    "- The `mode` which can set to be `local` is you want to run the experiment locally, or to `ec2` for launching the experiment on an Amazon Web Services instance. \n",
    "- The `seed` parameter which calibrates the randomness in the experiment. \n",
    "- The `tag` for your experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_tag = \"car-stabilizing-the-ring-local\"\n",
    "\n",
    "for seed in [5]:  # , 20, 68]:\n",
    "    run_experiment_lite(\n",
    "        run_task,\n",
    "        # Number of parallel workers for sampling\n",
    "        n_parallel=1,\n",
    "        # Keeps the snapshot parameters for all iterations\n",
    "        snapshot_mode=\"all\",\n",
    "        # Specifies the seed for the experiment. If this is not provided, a\n",
    "        # random seed will be used\n",
    "        seed=seed,\n",
    "        mode=\"local\",\n",
    "        exp_prefix=exp_tag,\n",
    "        # plot=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
