{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 04: Visualizing Experiment Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This tutorial describes the process of visualizing and replaying the results of Flow experiments run using RL. The process of visualizing results breaks down into two main components:\n",
    "\n",
    "- reward plotting\n",
    "\n",
    "- policy replay\n",
    "\n",
    "Furthermore, visualization is different depending on whether your experiments were run using rllab or RLlib. Accordingly, this tutorial is divided in two parts (one for rllab and one for RLlib)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rllab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An essential step in evaluating the effectiveness and training progress of RL agents is visualization of reward. rllab includes a tool to plot the _average cumulative reward per rollout_ against _iteration number_ to show training progress. This \"reward plot\" can be generated for just one experiment or many. The tool to be called is rllab's `frontend.py`, which is inside the directory `rllab/viskit/` (assuming a user is already inside the directory `rllab-multiagent`). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`frontend.py` requires only one command-line input: the path to the result directory that a user wants to visualize. The directory should contain a `progress.csv` and `params.json` fileâ€”pickle files containing per-iteration results are not necessary. An example call to `frontend.py` is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?! python rllab/viskit/frontend.py /path/to/result/directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replaying a Trained Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flow includes a tool for visualizing a trained policy in its environment using SUMO's GUI. This enables more granular analysis of policies beyond their accrued reward, which in turn allows users to tweak actions, observations, and rewards in order to produce desired behavior. The visualizer also generates plots of observations and a plot of reward over the course of the rollout. The tool to be called is `visualizer_rllab.py` within `flow/visualize` (assuming a user is already inside the parent directory `flow`). \n",
    "\n",
    "`visualizer_rllab.py` requires one command-line input and has three additional optional arguments. The required input is the path to the pickle file to be visualized (this is usually within an rllab result directory). The optional inputs are: \n",
    "- `--num_rollouts`, the number of rollouts to be visualized. The default value is 100. This argument takes integer input.\n",
    "- `--plotname`, the name of the plot generated by the visualizer. The default value is `traffic_plot`. This argument takes string input.\n",
    "- `--emission_to_csv`, whether to convert SUMO's emission file into a CSV file. Emission files will be discussed later in this tutorial. By default, emission CSV files are not stored. `--emission_to_csv` is a flag and takes no input.\n",
    "\n",
    "An example call to `visualizer_rllab.py` is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python flow/visualize/visualizer_rllab.py /path/to/result.pkl --num_rollouts 15 --plotname plot_test --emission_to_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Reward\n",
    "\n",
    "Similarly to how rllab handles reward plotting, RLlib supports reward visualization over the period of training using `tensorboard`. `tensorboard` takes one command-line input, `--logdir`, which is an rllib result directory (usually located within an experiment directory inside your `ray_results` directory). An example function call is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! tensorboard /ray_results/experiment_dir/result_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replaying a Trained Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! python flow/visualize/visualizer_rllib.py /path/to/result.pkl --num_rollouts 15 --plotname plot_test --emission_to_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data Collection and Analysis\n",
    "Any Flow experiment can output its results to a CSV file containing the contents of SUMO's built-in `emission.xml` files, specifying speed, position, time, fuel consumption, and many other metrics for all vehicles in a network over time. \n",
    "\n",
    "This section describes how to generate those emission CSV files when replaying a policy for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "- method of collecting data\n",
    "- emission_to_csv\n",
    "- lets you read in a lot of values and then plot using excel etc \n",
    "- where to find emission files for sumo experiments\n",
    "    - exp.run will contain that (to_csv param will output)\n",
    "\n",
    "### example\n",
    "run experiment, output CSV\n",
    "\n",
    "load CSV into pandas\n",
    "\n",
    "plot something briefly (spacetime?) (time-series trace)\n",
    "\n",
    "maybe sugiyama experiment, position traces\n",
    "\n",
    "### RL stuff\n",
    "visualize rllab, rllib experiment\n",
    "\n",
    "get CSV in both cases\n",
    "\n",
    "plot some time series again\n",
    "\n",
    "### maybe\n",
    "skip plots and say just where CSV goes and how to import it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
